import sqlite3
from datetime import datetime, timedelta
import threading
from queue import Queue
import json
import logging
import time
import requests
from typing import List, Dict
import os

class BatchVulnerabilityChecker:
    def __init__(self, cache_file=None):
        """Initialize the vulnerability checker with caching"""
        # Set up logging first
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            
        # Use OS-agnostic path for cache file
        if cache_file is None:
            cache_file = os.path.join(
                os.path.expanduser('~'),
                '.nodeheim',
                'vuln_cache.db'
            )
            os.makedirs(os.path.dirname(cache_file), exist_ok=True)
        
        self.cache_file = cache_file
        self.base_url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
        self.queue = Queue()
        self.results = {}
        self.worker_count = 3  # Default worker count
        
        # Initialize cache
        self.init_cache()
        
    def init_cache(self):
        """Initialize SQLite cache"""
        try:
            with sqlite3.connect(self.cache_file) as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS vulnerability_cache (
                        product TEXT,
                        version TEXT,
                        data JSON,
                        timestamp DATETIME,
                        PRIMARY KEY (product, version)
                    )
                """)
                self.logger.info(f"Initialized vulnerability cache at {self.cache_file}")
        except Exception as e:
            self.logger.error(f"Error initializing cache: {e}")
            raise
    
    def get_cached_vulns(self, product: str, version: str) -> dict:
        """Get vulnerabilities from cache if fresh"""
        try:
            with sqlite3.connect(self.cache_file) as conn:
                result = conn.execute("""
                    SELECT data, timestamp FROM vulnerability_cache 
                    WHERE product = ? AND version = ?
                """, (product, version)).fetchone()
                
                if result:
                    data, timestamp = result
                    cache_date = datetime.fromisoformat(timestamp)
                    # Cache valid for 7 days
                    if datetime.now() - cache_date < timedelta(days=7):
                        self.logger.debug(f"Cache hit for {product} {version}")
                        return json.loads(data)
                self.logger.debug(f"Cache miss for {product} {version}")
                return None
        except Exception as e:
            self.logger.error(f"Error reading from cache: {e}")
            return None

    def cache_vulns(self, product: str, version: str, data: dict):
        """Cache vulnerability data"""
        try:
            with sqlite3.connect(self.cache_file) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO vulnerability_cache (product, version, data, timestamp)
                    VALUES (?, ?, ?, ?)
                """, (product, version, json.dumps(data), datetime.now().isoformat()))
                self.logger.debug(f"Cached vulnerability data for {product} {version}")
        except Exception as e:
            self.logger.error(f"Error caching vulnerability data: {e}")

    def _worker(self):
        """Worker thread to process vulnerability lookups"""
        while True:
            try:
                product, version = self.queue.get()
                if product == "STOP":
                    break
                    
                # Check cache first
                cached_data = self.get_cached_vulns(product, version)
                if cached_data:
                    self.results[(product, version)] = cached_data
                    self.queue.task_done()
                    continue
                
                # Make API request with rate limiting
                time.sleep(0.6)  # Rate limit to ~100 requests/minute
                self.logger.info(f"Requesting vulnerabilities for {product} {version}")
                
                response = requests.get(
                    self.base_url,
                    params={
                        'keywordSearch': f"{product} {version}",
                        'resultsPerPage': 50  # Limit results to most relevant
                    },
                    headers={
                        'User-Agent': 'NodeheimSecurityScanner/1.0'
                    },
                    timeout=10
                )
                
                if response.status_code == 200:
                    data = self._process_vulnerability_data(response.json(), product, version)
                    self.cache_vulns(product, version, data)
                    self.results[(product, version)] = data
                else:
                    self.logger.error(f"API request failed: {response.status_code}")
                
                self.queue.task_done()
                
            except Exception as e:
                self.logger.error(f"Error in vulnerability worker: {e}")
                self.queue.task_done()

    def _process_vulnerability_data(self, raw_data: dict, product: str, version: str) -> dict:
        """Process raw vulnerability data"""
        try:
            vulns = raw_data.get('vulnerabilities', [])
            processed_vulns = []
            
            for vuln in vulns:
                cve_data = vuln.get('cve', {})
                metrics = cve_data.get('metrics', {}).get('cvssMetricV31', [{}])[0]
                
                # Get the most relevant description
                descriptions = cve_data.get('descriptions', [])
                description = next(
                    (d['value'] for d in descriptions if d.get('lang') == 'en'),
                    descriptions[0].get('value') if descriptions else 'No description available'
                )
                
                processed_vulns.append({
                    'cve_id': cve_data.get('id', ''),
                    'description': description,
                    'cvss_score': metrics.get('cvssData', {}).get('baseScore', 0),
                    'severity': metrics.get('cvssData', {}).get('baseSeverity', 'UNKNOWN'),
                    'attack_vector': metrics.get('cvssData', {}).get('attackVector', 'UNKNOWN'),
                    'attack_complexity': metrics.get('cvssData', {}).get('attackComplexity', 'UNKNOWN'),
                    'impact_score': metrics.get('impactScore', 0)
                })
            
            return {
                'product': product,
                'version': version,
                'vulnerability_count': len(processed_vulns),
                'vulnerabilities': sorted(processed_vulns, key=lambda x: x['cvss_score'], reverse=True),
                'max_cvss_score': max([v['cvss_score'] for v in processed_vulns], default=0),
                'last_updated': datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"Error processing vulnerability data: {e}")
            return {
                'product': product,
                'version': version,
                'vulnerability_count': 0,
                'vulnerabilities': [],
                'max_cvss_score': 0,
                'error': str(e)
            }

    def batch_check_services(self, services: List[Dict]) -> Dict[tuple, dict]:
        """Check vulnerabilities for multiple services efficiently"""
        try:
            self.logger.info(f"Starting batch vulnerability check for {len(services)} services")
            
            # Clear previous results
            self.results = {}
            
            # Start worker threads
            num_workers = 3  # Balance between speed and rate limits
            workers = []
            for i in range(num_workers):
                t = threading.Thread(target=self._worker)
                t.daemon = True  # Make thread daemon so it exits when main program does
                t.start()
                workers.append(t)
                self.logger.debug(f"Started worker thread {i+1}")
            
            # Queue all service checks
            for service in services:
                product = service.get('product', '')
                version = service.get('version', '')
                if product:  # Only queue if product name exists
                    self.queue.put((product, version))
            
            # Add stop signals for workers
            for _ in workers:
                self.queue.put(("STOP", ""))
            
            # Wait for completion
            self.queue.join()  # Wait for all tasks to complete
            for worker in workers:
                worker.join()
                
            self.logger.info(f"Completed vulnerability check. Found data for {len(self.results)} services")
            return self.results
            
        except Exception as e:
            self.logger.error(f"Error in batch vulnerability check: {e}")
            return {} 